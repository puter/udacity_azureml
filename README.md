# Optimizing an ML Pipeline in Azure - Chad Puterbaugh

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains marketing information for prospective customers. We seek to predict whether the customer will respond to a marketing campaign.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
Using hyperdrive, the best performing model was a logistic regression model with 2500 max iterations and a C value of 1.536. Its accuracy was .916

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The pipeline archecture was to create and register a tabular dataset from a url. That dataset was then converted into a pandas dataframe. The dataframe was then cleaned and prepared for training using one-hot encoding, as well as splitting the y values from the remaining x dataframes. The resulting x and y dataframes were split into test and training groups, then run through a logistic regression. The paramaters for the logistic regression training were fed dynamically using hyperdrive. Each run was scored per its classification accuracy, and the highest accuracy results were saved off into the outputs folder. 

**What are the benefits of the parameter sampler you chose?**
Exploring max_iter allowed the model to determine the ballpark iterations needed for the algorithm to converge on correct weights
**What are the benefits of the early stopping policy you chose?**
Compute time was of the essence for this experiment using free resources. The evaluation interval of 1 means that the policy is applied to every run, but at least the first half of the runs were evaluated first. The slack factor of .1 terminates any run within a very small degredation than the current candidate model. 

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best performing model was a Voting Ensemble model with accuracy of .918. The component models were XGBoost Classifier, SGD, and LightGBM, sometimes represented multiple times with differing parameters. 
## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
There was a small difference in the architecture of the two pipelines. I wanted to reuse as much of the code as I could, so I had to alter the training code to return the y_df in the results. The automl workflow when run remotely required that I registered the tabular dataset. That dataset also had to include the predictor. The automl workflow took the full timeline to run because I didn't restrict the solution space. AutoML performed much more accurately than the hyperdrive model. Hyperdrive only tried one classification algorithm, whereas automl explored the full solution space, and could arrive at a better model choice. The accuracy was only very marginally better, and to me not worth the extra execution time. 

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Future experiments could include taking learnings from automl in terms of better performing models and potentially allow it to run for longer or on a more powerful machine. I was constrained by budget for this experiement. Allowing automl to iterate over a larger solution space potentially could yield a model that otherwise I would not have been able to discover myself. 
